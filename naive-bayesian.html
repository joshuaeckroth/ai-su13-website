<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Naïve Bayesian classification</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="title" content="Naïve Bayesian classification"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2013-07-22 17:00:28 EDT"/>
<meta name="author" content="nil"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" href="css/worg.css" type="text/css" media="screen" />
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="MathJax/MathJax.js">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>

<div id="preamble">
<a href="index.html">Home</a> &nbsp; &nbsp;
          <!-- Plupper Button -->
          <div id="plupperButton" style="display: inline;"></div>
          <!-- End of Plupper Button Code -->

</div>

<div id="content">
<h1 class="title">Naïve Bayesian classification</h1>


<div class="summary"><h2>Summary</h2>
<ul>
<li>The &ldquo;naïve assumption&rdquo; is that the presence of a word does not give
  any information about the probability of the presence of any other
  word. This assumption gives us \(P(w_1, w_2, \dots|C_c) = \prod_i
  P(w_i|C_c)\).
</li>
<li>\(d_i\) is the number of documents (in the training set, of course)
  with word \(i\).
</li>
<li>\(d_{ci}\) is the number of documents in category \(C_c\) with the word
  \(i\).
</li>
<li>\(n_c\) is the number of documents in the category \(C_c\).
</li>
<li>\(n\) is the number of documents.
</li>
<li>\(|C|\) is the number of categories.
</li>
<li>\(P(C_c) = \frac{n_c + 1}{n + |C|}\) is the probability of any random
  document having category \(C_c\).
</li>
<li>\(\hat{X} = &lt;w_1, w_2, \dots, w_k&gt;\) is a binary document vector where
  each \(w_i \in \{0,1\}\).
</li>
<li>\(P(\hat{X}|C_c) = P(w_1, w_2, \dots, w_k | C_c) = \prod_i
  P(w_i|C_c)\) is the probability of the document having the words
  that it does assuming that the document comes from category \(C_c\).
</li>
<li>\(P(w_i|C_c) = \frac{d_{ci} + 1}{d_i + n_c}\) is the probability of a
  document from category \(C_c\) having word \(w_i\).
</li>
<li>\(\arg\max_{C_c} P(\hat{X}|C_c) P(C_c)\) is the question: which
  category \(C_c\) maximizes the probability that the document has these
  words?
</li>
<li>\(\arg\max_{C_c} \sum_i \log P(w_i|C_c) + \log P(C_c)\) is the new
  question using log-probability to avoid &ldquo;underflow&rdquo; in the
  floating-point values.
</li>
</ul>


</div>


<div id="table-of-contents">
<h2>TOC</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Feature vectors</a></li>
<li><a href="#sec-2">Category vectors</a></li>
<li><a href="#sec-3">Algorithm</a></li>
<li><a href="#sec-4">A problem with tiny values</a></li>
<li><a href="#sec-5">Evaluation</a></li>
<li><a href="#sec-6">Benefits of naïve Bayes</a></li>
<li><a href="#sec-7">Drawbacks of naïve Bayes</a></li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1">Feature vectors</h2>
<div class="outline-text-2" id="text-1">


<p>
Documents are simply binary word vectors. Each dimension in the vector
represents a unique word. The value in this dimension is 0 or 1
depending on whether the document has at least one occurrence of that
word.
</p>
</div>

</div>

<div id="outline-container-2" class="outline-2">
<h2 id="sec-2">Category vectors</h2>
<div class="outline-text-2" id="text-2">


<p>
Each category vector is represented as a series of probabilities, one
probability per word (each vector dimension represents a word, just
like a document feature vector). Each probability means, &ldquo;the
probability of this word being present in a document that is a member
of this category.&rdquo; Thus, the category vector has terms \(C_c = (p_{c1},
p_{c2}, \dots, p_{ck})\), and
</p>


$$p_{ci} = P(w_i|C_c) = \frac{d_{ci}+1}{d_i+n_c},$$

<p>
where \(d_{ci}\) is the number of documents in \(C_c\) that have word \(i\)
(anywhere in the document, any number of occurrences), \(d_i\) is the
number of documents in <i>any</i> category that have word \(i\), and \(n_c\) is
the number of documents in category \(C_c\). We add 1 and add \(n_c\) so
that \(P(w_i|C_c)\) is never equal to 0. (This is called <a href="http://en.wikipedia.org/wiki/Additive_smoothing">Laplace smoothing</a>.)
</p>
</div>

</div>

<div id="outline-container-3" class="outline-2">
<h2 id="sec-3">Algorithm</h2>
<div class="outline-text-2" id="text-3">


<p>
We assume, for simplicity, that the occurrences of words in documents
are completely independent (this is what makes the method
&ldquo;naïve&rdquo;). This is patently false since, for instance, the words
&ldquo;vision&rdquo; and &ldquo;image&rdquo; often both appear in documents about computer
vision; so seeing the word &ldquo;vision&rdquo; suggests that &ldquo;image&rdquo; will also
appear in the document.
</p>
<p>
We further assume that the order the words appear in the document does
not matter.
</p>
<p>
Because we make this independence assumption, we can calculate the
probability of a document being a member of some category quite
easily:
</p>


$$P(\hat{X}|C_c) = \prod_i P(w_i|C_c),$$

<p>
where \(P(w_i|C_c) = p_{ci}\) (from the definition above).
</p>
<p>
Now, Bayes&rsquo; theorem gives us:
</p>


$$P(C_c|\hat{X}) = P(\hat{X}|C_c)P(C_c) / P(\hat{X}),$$

<p>
with,
</p>


$$P(C_c) = \frac{n_c + 1}{n + |C|},$$

<p>
where \(n_c\) is the number of documents in category \(C_c\) and \(n\) is
the number of documents overall. Again, we use Laplace smoothing; this
allows us to avoid probabilities equal to 0.0.
</p>
<p>
Since we want to find the category \(C_c\) that makes the quantity
maximal, we can ignore \(P(\hat{X})\) because it does not change
depending on which category we are considering.
</p>
<p>
Thus, we are actually looking for:
</p>


$$\arg\max_{C_c} P(C_c|\hat{X}) = \arg\max_{C_c} P(\hat{X}|C_c)P(C_c)$$

<p>
We just check all the categories, and choose the single best or top \(N\).
</p>
</div>

</div>

<div id="outline-container-4" class="outline-2">
<h2 id="sec-4">A problem with tiny values</h2>
<div class="outline-text-2" id="text-4">


<p>
With a lot of unique words, we create very small values by multiplying
many \(p_{ci}\) terms. On a computer, the values may become so small
that they may &ldquo;underflow&rdquo; (run out of bits required to represent the
value). To prevent this, we just throw a logarithm around everything:
</p>


$$\log P(\hat{X}|C_c)P(C_c) = \log P(\hat{X}|C_c) + \log P(C_c),$$

<p>
and furthermore,
</p>


$$\log P(\hat{X}|C_c) = \log \prod_i P(w_i|C_c) = \sum_i \log P(w_i|C_c)$$

<p>
So our multiplications turn to sums, and we avoid the underflow
problem. Rewriting again, we ultimately have this problem:
</p>


$$\arg\max_{C_c} \sum_i \log P(w_i|C_c) + \log P(C_c)$$

</div>

</div>

<div id="outline-container-5" class="outline-2">
<h2 id="sec-5">Evaluation</h2>
<div class="outline-text-2" id="text-5">


<p>
The book <a href="http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html">Introduction to Information Retrieval</a> gathered some published
results for classification tasks. We can see that naïve Bayes is
usually not as good as k-nearest neighbor (which we did learn about)
nor support vector machines (which we didn&rsquo;t learn about).
</p>

<div style="text-align: center">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="right" /><col class="right" /><col class="right" />
</colgroup>
<thead>
<tr><th scope="col" class="left">Dataset</th><th scope="col" class="right">Naïve Bayes</th><th scope="col" class="right">k-nearest neighbor</th><th scope="col" class="right">Support vector machines</th></tr>
</thead>
<tbody>
<tr><td class="left">earn</td><td class="right">0.96</td><td class="right">0.97</td><td class="right">0.98</td></tr>
<tr><td class="left">acq</td><td class="right">0.88</td><td class="right">0.92</td><td class="right">0.94</td></tr>
<tr><td class="left">money-fx</td><td class="right">0.57</td><td class="right">0.78</td><td class="right">0.75</td></tr>
<tr><td class="left">grain</td><td class="right">0.79</td><td class="right">0.82</td><td class="right">0.95</td></tr>
<tr><td class="left">crude</td><td class="right">0.80</td><td class="right">0.86</td><td class="right">0.89</td></tr>
<tr><td class="left">trade</td><td class="right">0.64</td><td class="right">0.77</td><td class="right">0.76</td></tr>
<tr><td class="left">interest</td><td class="right">0.65</td><td class="right">0.74</td><td class="right">0.78</td></tr>
<tr><td class="left">ship</td><td class="right">0.85</td><td class="right">0.79</td><td class="right">0.86</td></tr>
<tr><td class="left">wheat</td><td class="right">0.70</td><td class="right">0.77</td><td class="right">0.92</td></tr>
<tr><td class="left">corn</td><td class="right">0.65</td><td class="right">0.78</td><td class="right">0.90</td></tr>
</tbody>
</table>


</div>

</div>

</div>

<div id="outline-container-6" class="outline-2">
<h2 id="sec-6">Benefits of naïve Bayes</h2>
<div class="outline-text-2" id="text-6">


<ul>
<li>It is very fast. In the table above, while naïve Bayes does not
    perform as well, it is significantly more efficient than either
    k-nearest neighbor or support vector machines. The latter, support
    vector machines, are <i>painfully</i> slow (at least in the training
    phase).
</li>
</ul>


</div>

</div>

<div id="outline-container-7" class="outline-2">
<h2 id="sec-7">Drawbacks of naïve Bayes</h2>
<div class="outline-text-2" id="text-7">


<ul>
<li>Accuracy is low, as seen in the table above.
</li>
</ul>





<div style="font-size: 80%; clear: both;"> <span
xmlns:dct="http://purl.org/dc/terms/"
href="http://purl.org/dc/dcmitype/Text" property="dct:title"
rel="dct:type">AI Su13 material</span> by <a
xmlns:cc="http://creativecommons.org/ns#"
href="http://ai-su13.artifice.cc" property="cc:attributionName"
rel="cc:attributionURL">Joshua Eckroth</a> is licensed under a <a
rel="license"
href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons
Attribution-ShareAlike 3.0 Unported License</a>. Source code for this
website available at <a
href="https://github.com/joshuaeckroth/ai-su13-website">GitHub</a>.
</div>

<!-- Plupper Tracking Code -->
<script src="https://www.google.com/jsapi"></script>
<script type="text/javascript"
    src="https://static.plupper.com/js/plupper.js"></script>
<script type="text/javascript">
    plupper.init("joshuaeckroth@plupper.com");
    plupper.enableCobrowsing();
</script>
<!-- End of Plupper Tracking Code -->

<script src="http://code.jquery.com/jquery-1.9.1.js"></script>
<script src="http://code.jquery.com/ui/1.10.3/jquery-ui.js"></script>

<script type="text/javascript">
$('.hidden .hidden-content').hide();
$('.hidden > strong').click(function() {
  $(this).parent().find('.hidden-content').toggle();
});
</script>


</div>
</div>
</div>

</body>
</html>
